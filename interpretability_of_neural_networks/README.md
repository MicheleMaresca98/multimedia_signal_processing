# interpretability_of_neural_networks
The paper concerns the interpretability of neural networks, that is to say, discovering what is hidden inside these "black boxes", that is, interpreting how the algorithms classify and correctly identify the problems under examination.
I illustrated CAM, an excellent interpretability technique introduced by Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva and Antonio Torralba in [Zhou et al., 2016], it allowed to correctly interpret the decisions made by neural networks allowing systems based on them to be more robust and more widespread in applications that have a strong impact on individuals in terms of financial, safety, or health. Later, I made an implementation of the CAM technique in Python, practically verifying the result obtained.
Through experiments I have examined the advantages introduced by this technique to neural networks in the classification and localization of images.
